# ğŸ§© What is Apache Spark?
Apache Spark is a distributed processing framework that allows you to manipulate large amounts of data in memory, quickly and scalably. In the context of this POC, Spark is used as the main ETL processing engineâ€”that is, it reads CSV/JSON files, applies transformations, and writes the processed output.

## ğŸ“¦ Initial project structure

````

poc-spark-etl/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ # Input files (CSV, JSON, etc.)
â”‚ â”‚ â””â”€â”€ sample_data.csv
â”‚ â””â”€â”€ output/           # Transformed data generated by Spark
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ main/
â”‚ â”‚ â””â”€â”€ etl_job.py       # Main ETL script
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ spark_session.py   # SparkSession creation (Spark initialization)
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ exploratory.ipynb  # (Optional) Exploratory analyses with Spark
â”‚
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
````

## âš™ï¸ Local Installation and Execution

###ğŸ”¹ Prerequisites

- Windows 11

- Java 17 (installed and configured)

- Python 3.13+

- pip installed

Check with:

###ğŸ”¹ 1. Clone the repository

````
git clone https://github.com/IcaroCaetano/poc-spark-etl.git
cd poc-spark-etl
````

###ğŸ”¹ 2. Create and activate the virtual environment (optional, but recommended)

````
python -m venv venv
venv\Scripts\activate
````

###ğŸ”¹ 3. Install dependencies

````
pip install -r requirements.txt
Contents of requirements.txt:

pyspark
pandas
pyarrow
````

###ğŸ”¹ 4. Run the ETL locally

````
py src/main/etl_job.py
````
