# ğŸ§© O que Ã© o Apache Spark

O Apache Spark Ã© um framework de processamento distribuÃ­do que permite manipular grandes quantidades de dados em memÃ³ria, de forma rÃ¡pida e escalÃ¡vel.
No contexto desta POC, o Spark Ã© usado como motor principal de processamento ETL â€” ou seja, ele lÃª arquivos CSV/JSON, aplica transformaÃ§Ãµes e grava a saÃ­da processada.

## ğŸ“¦ Estrutura inicial do projeto

````
poc-spark-etl/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                    # Arquivos de entrada (CSV, JSON, etc.)
â”‚   â”‚   â””â”€â”€ sample_data.csv
â”‚   â””â”€â”€ output/                   # Dados transformados gerados pelo Spark
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ etl_job.py            # Script principal de ETL
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ spark_session.py      # CriaÃ§Ã£o da SparkSession (inicializaÃ§Ã£o do Spark)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory.ipynb         # (Opcional) AnÃ¡lises exploratÃ³rias com Spark
â”‚
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
````

## âš™ï¸ InstalaÃ§Ã£o e execuÃ§Ã£o local
###ğŸ”¹ PrÃ©-requisitos

- Windows 11

- Java 17 (instalado e configurado)

- Python 3.13+

- pip instalado

Verifique com:

````
java -version
py --version
````

### ğŸ”¹ 1. Clonar o repositÃ³rio

````
git clone https://github.com/IcaroCaetano/poc-spark-etl.git
cd poc-spark-etl
````

### ğŸ”¹ 2. Criar e ativar o ambiente virtual (opcional, mas recomendado)

````
python -m venv venv
venv\Scripts\activate
````

### ğŸ”¹ 3. Instalar dependÃªncias

````
pip install -r requirements.txt
````

ConteÃºdo do requirements.txt:

````

pyspark
pandas
pyarrow
````

### ğŸ”¹ 4. Rodar o ETL localmente

````
py src/main/etl_job.py
````

âœ… Se tudo estiver configurado corretamente, o Spark iniciarÃ¡ e processarÃ¡ o arquivo data/input/sample_data.csv, gerando uma saÃ­da limpa em data/output/cleaned_data.parquet.

### ğŸ” O que Ã© o Apache Spark no contexto do seu projeto

O *Apache Spark* Ã© o motor de processamento distribuÃ­do responsÃ¡vel por executar o seu cÃ³digo ETL em paralelo.

No projeto, vocÃª nÃ£o executa o Spark diretamente: vocÃª interage com ele atravÃ©s do *PySpark*, a API Python oficial do Spark.

ğŸ‘‰ O *PySpark atua como uma ponte* entre o Python e o nÃºcleo do Spark (escrito em Scala/Java).

Quando vocÃª roda o script:

````
py src/main/etl_job.py
````

O PySpark:

1 - Inicializa o motor Apache Spark dentro da JVM (Java Virtual Machine).

2 - Cria um SparkContext que coordena o processamento.

3 - Executa as transformaÃ§Ãµes e aÃ§Ãµes em paralelo, mesmo em modo local.

## âš¡ Onde o Spark â€œentraâ€ no seu cÃ³digo
O Spark Ã© inicializado quando vocÃª cria uma SparkSession, como no trecho abaixo:

````
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("ETLExample")
    .master("local[*]")
    .getOrCreate()
)
````

- appName("ETLExample"): nome da aplicaÃ§Ã£o Spark.

- master("local[*]"): usa todos os nÃºcleos de CPU disponÃ­veis como â€œexecutorsâ€ Spark.

- getOrCreate(): inicializa o Spark localmente.

A partir daÃ­, todas as operaÃ§Ãµes (como spark.read.csv(), df.write.parquet(), etc.) sÃ£o executadas pelo motor distribuÃ­do do Apache Spark, e nÃ£o pelo Python puro.

