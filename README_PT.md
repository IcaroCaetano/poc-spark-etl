# ğŸ§© O que Ã© o Apache Spark

O Apache Spark Ã© um framework de processamento distribuÃ­do que permite manipular grandes quantidades de dados em memÃ³ria, de forma rÃ¡pida e escalÃ¡vel.
No contexto desta POC, o Spark Ã© usado como motor principal de processamento ETL â€” ou seja, ele lÃª arquivos CSV/JSON, aplica transformaÃ§Ãµes e grava a saÃ­da processada.

## ğŸ“¦ Estrutura inicial do projeto

````
poc-spark-etl/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                    # Arquivos de entrada (CSV, JSON, etc.)
â”‚   â”‚   â””â”€â”€ sample_data.csv
â”‚   â””â”€â”€ output/                   # Dados transformados gerados pelo Spark
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ etl_job.py            # Script principal de ETL
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ spark_session.py      # CriaÃ§Ã£o da SparkSession (inicializaÃ§Ã£o do Spark)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory.ipynb         # (Opcional) AnÃ¡lises exploratÃ³rias com Spark
â”‚
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
````

## âš™ï¸ InstalaÃ§Ã£o e execuÃ§Ã£o local
### ğŸ”¹ PrÃ©-requisitos

- Windows 11

- Java 17 (instalado e configurado)

- Python 3.13+

- pip instalado

Verifique com:

````
java -version
py --version
````

### ğŸ”¹ 1. Clonar o repositÃ³rio

````
git clone https://github.com/IcaroCaetano/poc-spark-etl.git
cd poc-spark-etl
````

### ğŸ”¹ 2. Criar e ativar o ambiente virtual (opcional, mas recomendado)

````
python -m venv venv
venv\Scripts\activate
````

### ğŸ”¹ 3. Instalar dependÃªncias

````
pip install -r requirements.txt
````

ConteÃºdo do requirements.txt:

````

pyspark
pandas
pyarrow
````

### ğŸ”¹ 4. Rodar o ETL localmente

````
py src/main/etl_job.py
````

âœ… Se tudo estiver configurado corretamente, o Spark iniciarÃ¡ e processarÃ¡ o arquivo data/input/sample_data.csv, gerando uma saÃ­da limpa em data/output/cleaned_data.parquet.

### ğŸ” O que Ã© o Apache Spark no contexto do seu projeto

O *Apache Spark* Ã© o motor de processamento distribuÃ­do responsÃ¡vel por executar o seu cÃ³digo ETL em paralelo.

No projeto, vocÃª nÃ£o executa o Spark diretamente: vocÃª interage com ele atravÃ©s do *PySpark*, a API Python oficial do Spark.

ğŸ‘‰ O *PySpark atua como uma ponte* entre o Python e o nÃºcleo do Spark (escrito em Scala/Java).

Quando vocÃª roda o script:

````
py src/main/etl_job.py
````

O PySpark:

1 - Inicializa o motor Apache Spark dentro da JVM (Java Virtual Machine).

2 - Cria um SparkContext que coordena o processamento.

3 - Executa as transformaÃ§Ãµes e aÃ§Ãµes em paralelo, mesmo em modo local.

## âš¡ Onde o Spark â€œentraâ€ no seu cÃ³digo
O Spark Ã© inicializado quando vocÃª cria uma SparkSession, como no trecho abaixo:

````
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("ETLExample")
    .master("local[*]")
    .getOrCreate()
)
````

- appName("ETLExample"): nome da aplicaÃ§Ã£o Spark.

- master("local[*]"): usa todos os nÃºcleos de CPU disponÃ­veis como â€œexecutorsâ€ Spark.

- getOrCreate(): inicializa o Spark localmente.

A partir daÃ­, todas as operaÃ§Ãµes (como spark.read.csv(), df.write.parquet(), etc.) sÃ£o executadas pelo motor distribuÃ­do do Apache Spark, e nÃ£o pelo Python puro.

## ğŸ§© Exemplo de como o Spark estÃ¡ processando seus dados

Trecho do etl_job.py:

````

df = spark.read.option("header", True).csv("data/input/sample_data.csv")
df_clean = df.na.drop().dropDuplicates()
df_clean.write.mode("overwrite").parquet("data/output/cleaned_data.parquet")
````

âœ¨ O que o Spark faz:
- ExtraÃ§Ã£o: lÃª o CSV de forma paralela.

- TransformaÃ§Ã£o: executa operaÃ§Ãµes (na.drop, dropDuplicates) em vÃ¡rios nÃºcleos da CPU.

- Carga: grava o resultado como Parquet em modo distribuÃ­do.

Mesmo no seu computador, o Spark simula um ambiente de cluster local, executando tarefas em paralelo.

## ğŸ§  Onde o Spark â€œmoraâ€ no seu ambiente
Ao instalar o PySpark com:

````
pip install pyspark
````
Ele instala o *Apache Spark* completo dentro da sua instalaÃ§Ã£o *Python*, normalmente em:

````
C:\Users\<seu-usuario>\AppData\Local\Programs\Python\Python313\Lib\site-packages\pyspark\
````

Ou seja, vocÃª jÃ¡ tem o Spark funcional dentro do seu ambiente Python â€” nÃ£o Ã© necessÃ¡rio baixar o binÃ¡rio separadamente para rodar localmente.

## ğŸ§© Resumo visual

````
Python script (etl_job.py)
        â†“
PySpark API (pyspark.sql, pyspark.ml, etc.)
        â†“
SparkSession â†’ inicializa o motor Apache Spark
        â†“
Spark executa transformaÃ§Ãµes (RDD/DataFrame) na JVM
        â†“
Resultados gravados em Parquet/CSV/DB, etc.
````

## ğŸ’¾ Exemplo de dados de entrada

Arquivo: data/input/sample_data.csv

````
id,name,age,city
1,Ana,25,SÃ£o Paulo
2,Bruno,30,Rio de Janeiro
3,Carlos,28,Belo Horizonte
4,Ana,25,SÃ£o Paulo
5,,27,Curitiba
````

ApÃ³s a execuÃ§Ã£o, o Spark irÃ¡:

- Remover registros duplicados.

- Remover linhas com valores nulos.

- Salvar a saÃ­da limpa como *Parquet* em data/output/cleaned_data.parquet.
